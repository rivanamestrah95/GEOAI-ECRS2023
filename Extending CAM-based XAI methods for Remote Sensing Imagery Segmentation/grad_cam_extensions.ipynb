{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rivanamestrah95/GEOAI-ECRS2023/blob/main/Extending%20CAM-based%20XAI%20methods%20for%20Remote%20Sensing%20Imagery%20Segmentation/grad_cam_extensions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "cfKnpWNfMb9L",
        "outputId": "4e8eeab3-ed14-42a8-ab41-d9f4a874bfd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=bf121a5e67920f07a137b3d78e15353b69dc541163213dda727c328a024c1a4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/0c/b3/674aea8c5d91c642c817d4d630bd58faa316724b136844094d\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "KHhEV9mm-TM5",
        "outputId": "2067a9ea-b51b-49ce-abd2-a4a054c5e424",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation_models_pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation_models_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.4.26)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation_models_pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation_models_pytorch-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "h3pUgy75-4S5",
        "outputId": "198f86a2-6015-4a90-b9e3-3466016785a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7a69OpET_Tn2",
        "outputId": "baff17c5-4790-43fa-e0a1-c292906bb97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/drive/MyDrive/Extending': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XEhTVd2IsqWN",
        "outputId": "976fe165-0449-44a7-d9d6-a5bb6245a900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'color_map'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-576302619f66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcolor_map\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'color_map'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import the required packages\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from segmentation_models_pytorch import Unet\n",
        "\n",
        "from skimage.io import imread,imsave\n",
        "import numpy as np\n",
        "import rasterio as rio\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "from utils import *\n",
        "from color_map import cm_data\n",
        "\n",
        "\n",
        "from rasterio.features import shapes\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation"
      ],
      "metadata": {
        "id": "V6h7UFypJ2Hr",
        "outputId": "4f13ae30-89b7-41d8-c9c1-560d85005583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJK_-IO_K4Nl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tyTJJS1RJbiN",
        "outputId": "9a960930-70df-4520-ac09-f54ba1ebae18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation"
      ],
      "metadata": {
        "id": "pUInD6b6K5o6",
        "outputId": "67daf926-ec2b-455e-8451-2d0fb9bc3bfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py\n"
      ],
      "metadata": {
        "id": "BMMY0XbUMpdK",
        "outputId": "f811b9b2-082a-48f7-8ec3-ebddb1146ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/utils.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_WO8qJq3sqWU",
        "outputId": "d7da9d19-ee2c-45bf-eb5f-2e7254bc6945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Unet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-15b7d6a76d93>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model = Unet(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mencoder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tu-tf_efficientnet_b0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_depth\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Unet' is not defined"
          ]
        }
      ],
      "source": [
        "# load your model with pre-trained model weights\n",
        "\n",
        "MODEL_PATH = './pretrained_model'\n",
        "THRESH = 0.5\n",
        "ALPHA = 0.85\n",
        "SCALE = None\n",
        "\n",
        "\n",
        "model = Unet(\n",
        "        encoder_name = \"tu-tf_efficientnet_b0\",\n",
        "        encoder_depth= 5,\n",
        "        encoder_weights = None,\n",
        "        decoder_use_batchnorm = True,\n",
        "        decoder_channels = (256, 128, 64, 32, 16),\n",
        "        decoder_attention_type = None,\n",
        "        in_channels= 3,\n",
        "        classes = 3,\n",
        "        activation = 'sigmoid',\n",
        "        aux_params = None,\n",
        "    )\n",
        "\n",
        "model = load_model(model,MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install rasterio\n",
        "!pip install geopandas\n",
        "!pip install scikit-image\n",
        "!pip install ttach\n",
        "!pip install imageio\n",
        "!pip install tqdm\n",
        "!pip install pytorch-grad-cam\n"
      ],
      "metadata": {
        "id": "FwThQFDuPBgH",
        "outputId": "2a9881a6-bb4a-4100-e838-600bcdf1e9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
            "Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam\n",
        "!pip install segmentation-models-pytorch\n",
        "!pip install ttach\n",
        "!pip install imageio\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install scikit-image\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "id": "nMG9lx7sPqyf",
        "outputId": "90ed8e3c-1e7f-47ca-9002-1de4732ab5f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam) (11.2.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.21.0+cu124)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=0c92eec7f82f86fc0833714e13c4f947723e5074a4d60a25c6d2c2848ba2552c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/52/78/893c3b94279ef238f43a9e89608af648de401b96415bebbd1f\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: grad-cam\n",
            "Successfully installed grad-cam-1.5.5\n",
            "Requirement already satisfied: segmentation-models-pytorch in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.2.1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas) (24.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (3.7.1)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.17.0)\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (1.4.3)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "d2Vq5fwOJTlu"
      },
      "outputs": [],
      "source": [
        "# Define the CAM-based Extensions classes\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import ttach as tta\n",
        "import sys\n",
        "import torch\n",
        "import warnings\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from segmentation_models_pytorch import Unet\n",
        "import argparse\n",
        "import os\n",
        "from typing import Callable, List\n",
        "import cv2\n",
        "import tqdm\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.measure import label\n",
        "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
        "from pytorch_grad_cam.utils.image import scale_cam_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.find_layers import replace_layer_recursive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "# Grad-Cam Classes\n",
        "class ActivationsAndGradients:\n",
        "    \"\"\" Class for extracting activations and\n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layers, reshape_transform):\n",
        "        self.model = model\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.handles = []\n",
        "        for target_layer in target_layers:\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_activation))\n",
        "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
        "            # we don't use backward hook to record gradients.\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_gradient))\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        activation = output\n",
        "        if self.reshape_transform is not None:\n",
        "            activation = self.reshape_transform(activation)\n",
        "        self.activations.append(activation.cpu().detach())\n",
        "\n",
        "    def save_gradient(self, module, input, output):\n",
        "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
        "            # You can only register hooks on tensor requires grad.\n",
        "            return\n",
        "\n",
        "        # Gradients are computed in reverse order\n",
        "        def _store_grad(grad):\n",
        "            if self.reshape_transform is not None:\n",
        "                grad = self.reshape_transform(grad)\n",
        "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
        "            # self.gradients = [torch.mul(t, -1) for t in self.gradients]\n",
        "\n",
        "        output.register_hook(_store_grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        return self.model(x)\n",
        "\n",
        "    def release(self):\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AblationLayer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AblationLayer, self).__init__()\n",
        "\n",
        "    def objectiveness_mask_from_svd(self, activations, threshold=0.01):\n",
        "        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n",
        "            The idea is to apply the EigenCAM method by doing PCA on the activations.\n",
        "            Then we create a binary mask by comparing to a low threshold.\n",
        "            Areas that are masked out, are probably not interesting anyway.\n",
        "        \"\"\"\n",
        "\n",
        "        projection = get_2d_projection(activations[None, :])[0, :]\n",
        "        projection = np.abs(projection)\n",
        "        projection = projection - projection.min()\n",
        "        projection = projection / projection.max()\n",
        "        projection = projection > threshold\n",
        "        return projection\n",
        "\n",
        "    def activations_to_be_ablated(\n",
        "            self,\n",
        "            activations,\n",
        "            ratio_channels_to_ablate=1.0):\n",
        "        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n",
        "            Create a binary CAM mask with objectiveness_mask_from_svd.\n",
        "            Score each Activation channel, by seeing how much of its values are inside the mask.\n",
        "            Then keep the top channels.\n",
        "\n",
        "        \"\"\"\n",
        "        if ratio_channels_to_ablate == 1.0:\n",
        "            self.indices = np.int32(range(activations.shape[0]))\n",
        "            return self.indices\n",
        "\n",
        "        projection = self.objectiveness_mask_from_svd(activations)\n",
        "\n",
        "        scores = []\n",
        "        for channel in activations:\n",
        "            normalized = np.abs(channel)\n",
        "            normalized = normalized - normalized.min()\n",
        "            normalized = normalized / np.max(normalized)\n",
        "            score = (projection * normalized).sum() / normalized.sum()\n",
        "            scores.append(score)\n",
        "        scores = np.float32(scores)\n",
        "\n",
        "        indices = list(np.argsort(scores))\n",
        "        high_score_indices = indices[::-\n",
        "                                     1][: int(len(indices) *\n",
        "                                              ratio_channels_to_ablate)]\n",
        "        low_score_indices = indices[: int(\n",
        "            len(indices) * ratio_channels_to_ablate)]\n",
        "        self.indices = np.int32(high_score_indices + low_score_indices)\n",
        "        return self.indices\n",
        "\n",
        "    def set_next_batch(\n",
        "            self,\n",
        "            input_batch_index,\n",
        "            activations,\n",
        "            num_channels_to_ablate):\n",
        "        \"\"\" This creates the next batch of activations from the layer.\n",
        "            Just take corresponding batch member from activations, and repeat it num_channels_to_ablate times.\n",
        "        \"\"\"\n",
        "        self.activations = activations[input_batch_index, :, :, :].clone(\n",
        "        ).unsqueeze(0).repeat(num_channels_to_ablate, 1, 1, 1)\n",
        "\n",
        "    def __call__(self, x, test=None):\n",
        "        output = self.activations\n",
        "        for i in range(output.size(0)):\n",
        "            # Commonly the minimum activation will be 0,\n",
        "            # And then it makes sense to zero it out.\n",
        "            # However depending on the architecture,\n",
        "            # If the values can be negative, we use very negative values\n",
        "            # to perform the ablation, deviating from the paper.\n",
        "            if torch.min(output) == 0:\n",
        "                output[i, self.indices[i], :] = 0\n",
        "            else:\n",
        "                ABLATION_VALUE = 1e7\n",
        "                output[i, self.indices[i], :] = torch.min(\n",
        "                    output) - ABLATION_VALUE\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GRADCAM_Extensions:\n",
        "    def __init__(self, extension, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n",
        "                 uses_gradients: bool = True) -> None:\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "        self.extension = extension\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "        # print('Activations list size: ', len(activations_list))\n",
        "        # print('Gradients list size: ', len(grads_list))\n",
        "        # print('Target layer size: ', len(self.target_layers))\n",
        "\n",
        "        cam_per_target_layer = []\n",
        "        # Loop over the saliency image from every layer\n",
        "        for i in range(len(self.target_layers)):\n",
        "            target_layer = self.target_layers[i]\n",
        "            # print('\\t\\t\\t-----------------------\\n')\n",
        "            # print('Target Layer ', i + 1, ': ', target_layer)\n",
        "            layer_activations = None\n",
        "            layer_grads = None\n",
        "            if i < len(activations_list):\n",
        "                layer_activations = activations_list[i]\n",
        "            if i < len(grads_list):\n",
        "                layer_grads = grads_list[i]\n",
        "\n",
        "\n",
        "            if  self.extension == \"grad_cam\":\n",
        "                weights = np.mean(layer_grads, axis=(2, 3))\n",
        "                weighs_up = weights[:, :, None, None]\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"hires_cam\":\n",
        "                elementwise_activations = layer_grads * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(elementwise_activations)\n",
        "                else:\n",
        "                    cam = elementwise_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"ew_cam\":\n",
        "                elementwise_activations = np.maximum(layer_grads * layer_activations, 0)\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(elementwise_activations)\n",
        "                else:\n",
        "                    cam = elementwise_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"grad_cam_pp\":\n",
        "                grads_power_2 = layer_grads**2\n",
        "                grads_power_3 = grads_power_2 * layer_grads\n",
        "                # Equation 19 in https://arxiv.org/abs/1710.11063\n",
        "                sum_activations = np.sum(layer_activations, axis=(2, 3))\n",
        "                eps = 0.000001\n",
        "                aij = grads_power_2 / (2 * grads_power_2 + sum_activations[:, :, None, None] * grads_power_3 + eps)\n",
        "                # Now bring back the ReLU from eq.7 in the paper,\n",
        "                # And zero out aijs where the activations are 0\n",
        "                aij = np.where(layer_grads != 0, aij, 0)\n",
        "                weights = np.maximum(layer_grads, 0) * aij\n",
        "                weights = np.sum(weights, axis=(2, 3))\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "            elif self.extension == \"x_grad_cam\":\n",
        "                sum_activations = np.sum(layer_activations, axis=(2, 3))\n",
        "                eps = 1e-7\n",
        "                weights = layer_grads * layer_activations / \\\n",
        "                (sum_activations[:, :, None, None] + eps)\n",
        "                weights = weights.sum(axis=(2, 3))\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "            elif self.extension == \"score_cam\":\n",
        "                with torch.no_grad():\n",
        "                    upsample = torch.nn.UpsamplingBilinear2d(size=input_tensor.shape[-2:])\n",
        "                    activation_tensor = torch.from_numpy(layer_activations)\n",
        "                    if self.cuda:\n",
        "                        activation_tensor = activation_tensor.cuda()\n",
        "                    upsampled = upsample(activation_tensor)\n",
        "                    maxs = upsampled.view(upsampled.size(0), upsampled.size(1), -1).max(dim=-1)[0]\n",
        "                    mins = upsampled.view(upsampled.size(0), upsampled.size(1), -1).min(dim=-1)[0]\n",
        "                    maxs, mins = maxs[:, :, None, None], mins[:, :, None, None]\n",
        "                    upsampled = (upsampled - mins) / (maxs - mins)\n",
        "\n",
        "                    input_tensors = input_tensor[:, None, :, :] * upsampled[:, :, None, :, :]\n",
        "                    if hasattr(self, \"batch_size\"):\n",
        "                        BATCH_SIZE = self.batch_size\n",
        "                    else:\n",
        "                        BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "                    scores = []\n",
        "                    for target, tensor in zip(targets, input_tensors):\n",
        "                        for i in tqdm.tqdm(range(0, tensor.size(0), BATCH_SIZE)):\n",
        "                            batch = tensor[i: i + BATCH_SIZE, :]\n",
        "                            outputs = [target(o).cpu().item() for o in self.model(batch)]\n",
        "                            scores.extend(outputs)\n",
        "                    scores = torch.Tensor(scores)\n",
        "                    scores = scores.view(layer_activations.shape[0], layer_activations.shape[1])\n",
        "                    weights = torch.nn.Softmax(dim=-1)(scores).numpy()\n",
        "\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"layer_cam\":\n",
        "                spatial_weighted_activations = np.maximum(layer_grads, 0) * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(spatial_weighted_activations)\n",
        "                else:\n",
        "                    cam = spatial_weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "            elif self.extension == \"eigen_cam\":\n",
        "                cam = get_2d_projection(layer_activations)\n",
        "\n",
        "            elif self.extension == \"eigen_grad_cam\":\n",
        "                cam = get_2d_projection(layer_grads * layer_activations)\n",
        "\n",
        "            else:\n",
        "                print(\"Unkown Extension. Please use one of the following: grad_cam - hires_cam - ew_cam -  grad_cam_pp - x_grad_cam - score_cam - layer_cam - eigen_cam -  eigen_grad_cam\")\n",
        "\n",
        "\n",
        "\n",
        "            cam = np.maximum(cam, 0)\n",
        "            # print(\"Cam image  Max per layer size: \", cam.shape)\n",
        "            scaled = scale_cam_image(cam, target_size)\n",
        "            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n",
        "            cam_per_target_layer.append(scaled[:, None, :])\n",
        "\n",
        "        # print(\"Cam image list size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
        "        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n",
        "        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n",
        "        # print(\"+++ Averaged CAM list size: \", result.shape)\n",
        "\n",
        "\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GRADCAMEXTENDED_AblationCAM:\n",
        "    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n",
        "                 uses_gradients: bool = True,\n",
        "                 ablation_layer: torch.nn.Module = AblationLayer(),\n",
        "                 batch_size: int = 32,\n",
        "                 ratio_channels_to_ablate: float = 1.0) -> None:\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "        self.batch_size = batch_size\n",
        "        self.ablation_layer = ablation_layer\n",
        "        self.ratio_channels_to_ablate = ratio_channels_to_ablate\n",
        "\n",
        "\n",
        "    def save_activation(self, module, input, output) -> None:\n",
        "        \"\"\" Helper function to save the raw activations from the target layer \"\"\"\n",
        "        self.activations = output\n",
        "\n",
        "    def assemble_ablation_scores(self,\n",
        "                                 new_scores: list,\n",
        "                                 original_score: float,\n",
        "                                 ablated_channels: np.ndarray,\n",
        "                                 number_of_channels: int) -> np.ndarray:\n",
        "        \"\"\" Take the value from the channels that were ablated,\n",
        "            and just set the original score for the channels that were skipped \"\"\"\n",
        "\n",
        "        index = 0\n",
        "        result = []\n",
        "        sorted_indices = np.argsort(ablated_channels)\n",
        "        ablated_channels = ablated_channels[sorted_indices]\n",
        "        new_scores = np.float32(new_scores)[sorted_indices]\n",
        "\n",
        "        for i in range(number_of_channels):\n",
        "            if index < len(ablated_channels) and ablated_channels[index] == i:\n",
        "                weight = new_scores[index]\n",
        "                index = index + 1\n",
        "            else:\n",
        "                weight = original_score\n",
        "            result.append(weight)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "        # print('Activations list size: ', len(activations_list))\n",
        "        # print('Gradients list size: ', len(grads_list))\n",
        "        # print('Target layer size: ', len(self.target_layers))\n",
        "\n",
        "        cam_per_target_layer = []\n",
        "        # Loop over the saliency image from every layer\n",
        "        for i in range(len(self.target_layers)):\n",
        "            target_layer = self.target_layers[i]\n",
        "            # print('\\t\\t\\t-----------------------\\n')\n",
        "            # print('Target Layer ', i + 1, ': ', target_layer)\n",
        "            layer_activations = None\n",
        "            layer_grads = None\n",
        "            if i < len(activations_list):\n",
        "                layer_activations = activations_list[i]\n",
        "            if i < len(grads_list):\n",
        "                layer_grads = grads_list[i]\n",
        "\n",
        "\n",
        "            # get weights\n",
        "            # Do a forward pass, compute the target scores, and cache the\n",
        "            # activations\n",
        "            handle = target_layer.register_forward_hook(self.save_activation)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_tensor)\n",
        "                handle.remove()\n",
        "                original_scores = np.float32(\n",
        "                    [target(output).cpu().item() for target, output in zip(targets, outputs)])\n",
        "\n",
        "            # Replace the layer with the ablation layer.\n",
        "            # When we finish, we will replace it back, so the original model is\n",
        "            # unchanged.\n",
        "            ablation_layer = self.ablation_layer\n",
        "            replace_layer_recursive(self.model, target_layer, ablation_layer)\n",
        "\n",
        "            number_of_channels = layer_activations.shape[1]\n",
        "            weights = []\n",
        "            # This is a \"gradient free\" method, so we don't need gradients here.\n",
        "            with torch.no_grad():\n",
        "                # Loop over each of the batch images and ablate activations for it.\n",
        "                for batch_index, (target, tensor) in enumerate(\n",
        "                        zip(targets, input_tensor)):\n",
        "                    new_scores = []\n",
        "                    batch_tensor = tensor.repeat(self.batch_size, 1, 1, 1)\n",
        "\n",
        "                    # Check which channels should be ablated. Normally this will be all channels,\n",
        "                    # But we can also try to speed this up by using a low\n",
        "                    # ratio_channels_to_ablate.\n",
        "                    channels_to_ablate = ablation_layer.activations_to_be_ablated(\n",
        "                        layer_activations[batch_index, :], self.ratio_channels_to_ablate)\n",
        "                    number_channels_to_ablate = len(channels_to_ablate)\n",
        "\n",
        "                    for i in tqdm.tqdm(range(0, number_channels_to_ablate, self.batch_size)):\n",
        "                        if i + self.batch_size > number_channels_to_ablate:\n",
        "                            batch_tensor = batch_tensor[:(number_channels_to_ablate - i)]\n",
        "\n",
        "                        # Change the state of the ablation layer so it ablates the next channels.\n",
        "                        # TBD: Move this into the ablation layer forward pass.\n",
        "                        ablation_layer.set_next_batch(input_batch_index=batch_index,activations=self.activations,\n",
        "                            num_channels_to_ablate=batch_tensor.size(0))\n",
        "\n",
        "\n",
        "                        score = [target(o).cpu().item() for o in self.model(batch_tensor)]\n",
        "                        new_scores.extend(score)\n",
        "\n",
        "                        ablation_layer.indices = ablation_layer.indices[batch_tensor.size(0):]\n",
        "\n",
        "\n",
        "                    new_scores = self.assemble_ablation_scores(new_scores,original_scores[batch_index], channels_to_ablate,\n",
        "                        number_of_channels)\n",
        "                    weights.extend(new_scores)\n",
        "\n",
        "            weights = np.float32(weights)\n",
        "            weights = weights.reshape(layer_activations.shape[:2])\n",
        "            original_scores = original_scores[:, None]\n",
        "            weights = (original_scores - weights) / original_scores\n",
        "\n",
        "            # Replace the model back to the original state\n",
        "            #-----------------------------------\n",
        "            replace_layer_recursive(self.model, ablation_layer, target_layer)\n",
        "\n",
        "\n",
        "            # Equation 3.1\n",
        "            weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "\n",
        "            # Equation 3.2\n",
        "            if eigen_smooth:\n",
        "                cam = get_2d_projection(weighted_activations)\n",
        "            else:\n",
        "                cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            cam = np.maximum(cam, 0)\n",
        "            # print(\"Cam image  Max per layer size: \", cam.shape)\n",
        "            scaled = scale_cam_image(cam, target_size)\n",
        "            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n",
        "            cam_per_target_layer.append(scaled[:, None, :])\n",
        "\n",
        "        # print(\"Cam image list size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
        "        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n",
        "        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n",
        "        # print(\"+++ Averaged CAM list size: \", result.shape)\n",
        "\n",
        "\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pytorch_grad_cam.utils.find_layers import find_layer_predicate_recursive\n",
        "from pytorch_grad_cam.utils.image import scale_accross_batch_and_channels\n",
        "\n",
        "\n",
        "class GRADCAMEXTENDED_FullGrad:\n",
        "    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = True,\n",
        "                 uses_gradients: bool = True) -> None:\n",
        "\n",
        "        if len(target_layers) > 0:\n",
        "            print(\n",
        "                \"Warning: target_layers is ignored in FullGrad. All bias layers will be used instead\")\n",
        "\n",
        "        def layer_with_2D_bias(layer):\n",
        "            bias_target_layers = [torch.nn.Conv2d, torch.nn.BatchNorm2d]\n",
        "            if type(layer) in bias_target_layers and layer.bias is not None:\n",
        "                return True\n",
        "            return False\n",
        "        target_layers = find_layer_predicate_recursive(model, layer_with_2D_bias)\n",
        "        self.bias_data = [self.get_bias_data(layer).cpu().numpy() for layer in target_layers]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "\n",
        "\n",
        "    def get_bias_data(self, layer):\n",
        "        # Borrowed from official paper impl:\n",
        "        # https://github.com/idiap/fullgrad-saliency/blob/master/saliency/tensor_extractor.py#L47\n",
        "        if isinstance(layer, torch.nn.BatchNorm2d):\n",
        "            bias = - (layer.running_mean * layer.weight\n",
        "                      / torch.sqrt(layer.running_var + layer.eps)) + layer.bias\n",
        "            return bias.data\n",
        "        else:\n",
        "            return layer.bias.data\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        #----remove------\n",
        "\n",
        "        input_grad = input_tensor.grad.data.cpu().numpy()\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        cam_per_target_layer = []\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "\n",
        "        gradient_multiplied_input = input_grad * input_tensor.data.cpu().numpy()\n",
        "        gradient_multiplied_input = np.abs(gradient_multiplied_input)\n",
        "        gradient_multiplied_input = scale_accross_batch_and_channels(\n",
        "            gradient_multiplied_input,\n",
        "            target_size)\n",
        "        cam_per_target_layer.append(gradient_multiplied_input)\n",
        "\n",
        "        # Loop over the saliency image from every layer\n",
        "        assert(len(self.bias_data) == len(grads_list))\n",
        "        for bias, grads in zip(self.bias_data, grads_list):\n",
        "            bias = bias[None, :, None, None]\n",
        "            # In the paper they take the absolute value,\n",
        "            # but possibily taking only the positive gradients will work\n",
        "            # better.\n",
        "            bias_grad = np.abs(bias * grads)\n",
        "            result = scale_accross_batch_and_channels(\n",
        "                bias_grad, target_size)\n",
        "            result = np.sum(result, axis=1)\n",
        "            cam_per_target_layer.append(result[:, None, :])\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        if eigen_smooth:\n",
        "            # Resize to a smaller image, since this method typically has a very large number of channels,\n",
        "            # and then consumes a lot of memory\n",
        "            cam_per_target_layer = scale_accross_batch_and_channels(\n",
        "                cam_per_target_layer, (target_size[0] // 8, target_size[1] // 8))\n",
        "            cam_per_target_layer = get_2d_projection(cam_per_target_layer)\n",
        "            cam_per_target_layer = cam_per_target_layer[:, None, :, :]\n",
        "            cam_per_target_layer = scale_accross_batch_and_channels(\n",
        "                cam_per_target_layer,\n",
        "                target_size)\n",
        "        else:\n",
        "            cam_per_target_layer = np.sum(\n",
        "                cam_per_target_layer, axis=1)[:, None, :]\n",
        "\n",
        "\n",
        "        result = np.sum(cam_per_target_layer, axis=1)\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "editable": true,
        "id": "8qvw_iUTsqWh",
        "tags": [],
        "outputId": "6c0a272d-94d4-4266-abdf-28fb8e43c48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'2_566.png' b'2_566.png'\n",
            "Testing Image:  1\n",
            "Input Image Shape:  (512, 512, 3)\n",
            "GT mask shape:  (512, 512)\n",
            "predicated tensor shape:  torch.Size([3, 512, 512])\n",
            "Model Target Confidence Score:  0.9124947\n",
            "Model Target Entropy Score:  0.0014297119\n",
            "running seg-grad-cam...\n",
            "SGC Target Confidence Score:  0.03352213  Entropy Score:  0.001117439\n",
            "running GradCam++ ...\n",
            "SGC-PP Target Confidence Score:  0.037998214  Entropy Score:  0.0011307257\n",
            "running XGradCam...\n",
            "SGC-X Target Confidence Score:  0.033650283  Entropy Score:  0.001116565\n",
            "running ScoreCam...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [10:31<00:00, 19.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGC-Score Target Confidence Score:  0.7579551  Entropy Score:  0.0019834307\n",
            "running EigenCam...\n",
            "SGC-eigen Target Confidence Score:  0.04277919  Entropy Score:  0.0013071129\n",
            "running AblationCAM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/8 [01:05<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "AblationLayer.__call__() got an unexpected keyword argument 'skip_connection'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9afc1f060a24>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running AblationCAM...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mGRADCAMEXTENDED_AblationCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mablcam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                     \u001b[0mgrayscale_cam_EX_abl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mablcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mExmap_ablsgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap_ablsgc_rgba\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_cam_image_whu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_img_rgba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale_cam_EX_abl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mconfidence_asgc_M2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy_asgc_M2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbw_sgca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE_sgca\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXAI_EVAL_M2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrayscale_cam_EX_abl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_img_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrrp_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_category\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-038b79be5ed5>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigen_smooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-038b79be5ed5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                         \u001b[0mnew_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/decoders/unet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mskip_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_connections\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_connection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: AblationLayer.__call__() got an unexpected keyword argument 'skip_connection'"
          ]
        }
      ],
      "source": [
        "# Apply the adapted CAM-based Extensions on the considered dataset\n",
        "\n",
        "class SemanticSegmentationTarget:\n",
        "    def __init__(self, category, mask):\n",
        "        self.category = category\n",
        "        self.mask = torch.from_numpy(mask)\n",
        "        if torch.cuda.is_available():\n",
        "            self.mask = self.mask.cuda()\n",
        "\n",
        "    def __call__(self, model_output):\n",
        "        return (model_output[self.category, :, : ] * self.mask).sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def defaultScales():\n",
        "    classes_cmap = plt.get_cmap('Spectral', 20)\n",
        "    scale_fig = 2\n",
        "    fonts = 15\n",
        "    scatter_size = 330 * scale_fig\n",
        "    return classes_cmap, scale_fig, fonts, scatter_size\n",
        "\n",
        "\n",
        "def show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight):\n",
        "    heatmap_sgc = cv2.applyColorMap(np.uint8(255 * grayscale_cam_EX), cv2.COLORMAP_JET)\n",
        "    heatmap_sgc = cv2.cvtColor(heatmap_sgc, cv2.COLOR_BGR2RGB)\n",
        "    heatmap_sgc = np.float32(heatmap_sgc) / 255\n",
        "    if full_img.shape[-1]==4:\n",
        "        heatmap_sgc = cv2.cvtColor(heatmap_sgc,cv2.COLOR_RGB2RGBA)\n",
        "    Exmap_sgc = (1 - image_weight) * heatmap_sgc + image_weight * full_img_rgba\n",
        "    Exmap_sgc = Exmap_sgc / np.max(Exmap_sgc)\n",
        "    Exmap_sgc = np.uint8(255 * Exmap_sgc)\n",
        "\n",
        "    return Exmap_sgc, heatmap_sgc\n",
        "\n",
        "\n",
        "def prob_2_entropy(prob):\n",
        "    \"\"\" convert probabilistic prediction maps to weighted self-information maps\n",
        "    \"\"\"\n",
        "    n, c, h, w = prob.size()\n",
        "    return -torch.mul(prob, torch.log2(prob + 1e-30)) / np.log2(c)\n",
        "\n",
        "def XAI_EVAL_M2(thres,grayscale_cam_EX, full_img_gt, full_img, model, rrp_info,target_category):\n",
        "    im_bw_sgc = cv2.threshold(grayscale_cam_EX, thres, 1, cv2.THRESH_BINARY)[1]\n",
        "    union_gt_sgc = np.ma.mask_or(full_img_gt,im_bw_sgc)\n",
        "\n",
        "    E_sgc = full_img * union_gt_sgc[..., None]\n",
        "    x_sgc = totensor(E_sgc)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred_sgc = model(x_sgc)\n",
        "        y_pred_sgc = unpad_resize(y_pred_sgc,rrp_info)\n",
        "\n",
        "    mask_tensor_sgc = y_pred_sgc[0,...]\n",
        "    mask_sgc = y_pred_sgc[0,...].cpu().numpy().transpose(1,2,0)\n",
        "    target_mask_f = np.float32(mask_sgc[:,:,target_category]) * full_img_gt\n",
        "    target_Confidence_score = target_mask_f[np.nonzero(target_mask_f)]\n",
        "    target_Confidence_score_sgc = np.mean(target_Confidence_score)\n",
        "\n",
        "    logist_softmax_entropy_sgc = prob_2_entropy(y_pred_sgc)\n",
        "    target_entropy_mask_sgc = logist_softmax_entropy_sgc[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n",
        "    target_entropy_mask_class_sgc = target_entropy_mask_sgc[:,:,target_category] * full_img_gt\n",
        "    target_entropy_mask_class_sgc_sc = (np.mean(target_entropy_mask_class_sgc))\n",
        "    return target_Confidence_score_sgc, target_entropy_mask_class_sgc_sc, im_bw_sgc,E_sgc\n",
        "\n",
        "\n",
        "\n",
        "images_dir = r'./dataset/images'\n",
        "gt_dir = r'./dataset/gt'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ticks = np.linspace(0, 1, 6, endpoint=True)\n",
        "classes_cmap, scale_fig, fonts, scatter_size = defaultScales()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_idx = 1\n",
        "n_xai = 6\n",
        "n_imgs = 1\n",
        "target_layers =  [model.decoder.blocks[decoder_idx - 1]]\n",
        "target_category = 0\n",
        "XAI_method = [\"grad_cam\", \"hires_cam\", \"ew_cam\", \"grad_cam_pp\", \"x_grad_cam\",\"score_cam\", \"layer_cam\", \"eigen_cam\", \"eigen_grad_cam\"]\n",
        "\n",
        "image_weight = 0.006\n",
        "\n",
        "directory_images = os.fsencode(images_dir)\n",
        "directory_gt = os.fsencode(gt_dir)\n",
        "\n",
        "\n",
        "number_testing_images = 0\n",
        "\n",
        "Model_Seg_Score = np.zeros((n_imgs))\n",
        "Model_Seg_Entropy = np.zeros((n_imgs))\n",
        "\n",
        "Seg_Score = np.zeros((n_xai, n_imgs))\n",
        "Seg_Entropy = np.zeros((n_xai, n_imgs))\n",
        "counter = 0\n",
        "\n",
        "\n",
        "for (file_img, file_gt) in zip(os.listdir(directory_images), os.listdir(directory_gt) ):\n",
        "\n",
        "    print(file_img, file_gt)\n",
        "    if file_img == file_gt:\n",
        "        number_testing_images = number_testing_images + 1\n",
        "        print(\"Testing Image: \", number_testing_images)\n",
        "        filename = os.fsdecode(file_img)\n",
        "        filename_gt = os.fsdecode(file_gt)\n",
        "        if filename.endswith(\".png\") and filename_gt.endswith(\".png\"):\n",
        "            raster_file = rio.open(f'{images_dir}/{filename}')\n",
        "            full_img = raster_file.read().transpose(1,2,0)\n",
        "            full_img,rrp_info = ratio_resize_pad(full_img, ratio = None)\n",
        "            full_img_rgba = full_img\n",
        "            print(\"Input Image Shape: \", full_img.shape)\n",
        "            if full_img.shape[-1]==4:full_img = cv2.cvtColor(full_img,cv2.COLOR_RGBA2RGB) #WHU images are RGBA\n",
        "            # read gt mask\n",
        "            raster_file_gt = rio.open(f'{gt_dir}/{filename_gt}')\n",
        "            full_img_gt = raster_file_gt.read().transpose(1,2,0)\n",
        "            full_img_gt,rrp_info_gt = ratio_resize_pad(full_img_gt, ratio = None)\n",
        "            full_img_gt = np.float32(full_img_gt) / np.max(full_img_gt)\n",
        "            print(\"GT mask shape: \", full_img_gt.shape)\n",
        "\n",
        "\n",
        "            full_img = normalize(full_img)\n",
        "            x = totensor(full_img)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(x)\n",
        "                y_pred = unpad_resize(y_pred,rrp_info)\n",
        "\n",
        "\n",
        "\n",
        "            mask_tensor = y_pred[0,...]\n",
        "            print('predicated tensor shape: ', mask_tensor.shape)\n",
        "            mask = y_pred[0,...].cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "            target_mask_float = np.float32(mask[:,:,target_category]) * full_img_gt\n",
        "            target_Confidence_score = target_mask_float[np.nonzero(target_mask_float)]\n",
        "            target_Confidence_score = np.mean(target_Confidence_score)\n",
        "            print(\"Model Target Confidence Score: \", target_Confidence_score)\n",
        "\n",
        "            logist_softmax_entropy = prob_2_entropy(y_pred)\n",
        "            target_entropy_mask = logist_softmax_entropy[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n",
        "            target_entropy_mask_class = target_entropy_mask[:,:,target_category] * full_img_gt\n",
        "            model_entropy = np.mean(target_entropy_mask_class)\n",
        "            print(\"Model Target Entropy Score: \", model_entropy )\n",
        "\n",
        "\n",
        "            targets = [SemanticSegmentationTarget(target_category, target_mask_float)]\n",
        "\n",
        "            nan_condition = np.count_nonzero(target_mask_float)\n",
        "            if nan_condition != 0:\n",
        "\n",
        "                print('running seg-grad-cam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[0], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n",
        "                    grayscale_cam_EX = cam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_sgc, heatmap_sgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight)\n",
        "                    [confidence_sgc_M2, entropy_sgc_M2, bw_sgc,E_sgc] = XAI_EVAL_M2(0.4,grayscale_cam_EX, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC Target Confidence Score: \", confidence_sgc_M2,\" Entropy Score: \", entropy_sgc_M2)\n",
        "\n",
        "                print('running GradCam++ ...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[3], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as campp:\n",
        "                    grayscale_cam_EX_Plusplus = campp(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_sgcpp, heatmap_sgcpp_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Plusplus, image_weight)\n",
        "                    [confidence_sgcpp_M2, entropy_sgcpp_M2, bw_sgcpp,E_sgcpp] = XAI_EVAL_M2(0.4,grayscale_cam_EX_Plusplus, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-PP Target Confidence Score: \", confidence_sgcpp_M2,\" Entropy Score: \", entropy_sgcpp_M2)\n",
        "\n",
        "                print('running XGradCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[4], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as xcam:\n",
        "                    grayscale_cam_EX_X = xcam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_Xsgc, heatmap_Xsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_X, image_weight)\n",
        "                    [confidence_Xsgc_M2, entropy_Xsgc_M2, bw_sgcx,E_sgcx] = XAI_EVAL_M2(0.4,grayscale_cam_EX_X, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-X Target Confidence Score: \", confidence_Xsgc_M2,\" Entropy Score: \", entropy_Xsgc_M2)\n",
        "\n",
        "\n",
        "                print('running ScoreCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[5], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as scorecam:\n",
        "                    grayscale_cam_EX_Score = scorecam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_scoresgc, heatmap_scoresgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Score, image_weight)\n",
        "                    [confidence_ssgc_M2, entropy_ssgc_M2, bw_sgcs,E_sgcs] = XAI_EVAL_M2(0.4,grayscale_cam_EX_Score, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-Score Target Confidence Score: \", confidence_ssgc_M2,\" Entropy Score: \", entropy_ssgc_M2)\n",
        "\n",
        "\n",
        "                print('running EigenCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[7], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ecam:\n",
        "                    grayscale_cam_EX_eigen = ecam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_eigensgc, heatmap_eigensgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_eigen, image_weight)\n",
        "                    [confidence_esgc_M2, entropy_esgc_M2, bw_sgce, E_sgce] = XAI_EVAL_M2(0.4,grayscale_cam_EX_eigen, full_img_gt, full_img, model, rrp_info,target_category)\n",
        "                    print(\"SGC-eigen Target Confidence Score: \", confidence_esgc_M2,\" Entropy Score: \", entropy_esgc_M2)\n",
        "\n",
        "\n",
        "\n",
        "                print('running AblationCAM...')\n",
        "                with GRADCAMEXTENDED_AblationCAM(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ablcam:\n",
        "                    grayscale_cam_EX_abl = ablcam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_ablsgc, heatmap_ablsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_abl, image_weight)\n",
        "                    [confidence_asgc_M2, entropy_asgc_M2, bw_sgca, E_sgca] = XAI_EVAL_M2(0.4,grayscale_cam_EX_abl, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-Ablbation Target Confidence Score: \", confidence_asgc_M2,\" Entropy Score: \", entropy_asgc_M2)\n",
        "\n",
        "\n",
        "\n",
        "                Model_Seg_Score[counter] = target_Confidence_score\n",
        "                Model_Seg_Entropy[counter] = model_entropy\n",
        "\n",
        "                Seg_Score[0,counter] = confidence_sgc_M2\n",
        "                Seg_Score[1,counter] = confidence_sgcpp_M2\n",
        "                Seg_Score[2,counter] = confidence_Xsgc_M2\n",
        "                Seg_Score[3,counter] = confidence_ssgc_M2\n",
        "                Seg_Score[4,counter] = confidence_esgc_M2\n",
        "                Seg_Score[5,counter] = confidence_asgc_M2\n",
        "\n",
        "                Seg_Entropy[0,counter] = entropy_sgc_M2\n",
        "                Seg_Entropy[1,counter] = entropy_sgcpp_M2\n",
        "                Seg_Entropy[2,counter] = entropy_Xsgc_M2\n",
        "                Seg_Entropy[3,counter] = entropy_ssgc_M2\n",
        "                Seg_Entropy[4,counter] = entropy_esgc_M2\n",
        "                Seg_Entropy[5,counter] = entropy_asgc_M2\n",
        "                counter = counter + 1\n",
        "\n",
        "                if(number_testing_images == 2):\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n",
        "\n",
        "                plt.subplot(3,3,1)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(full_img_rgba)\n",
        "                plt.title('Input image', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,2)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n",
        "                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,3)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n",
        "                plt.title('Predicted Mask', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,4)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_sgc)\n",
        "                plt.title('Seg-Grad-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,5)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_sgcpp)\n",
        "                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,6)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_Xsgc)\n",
        "                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,7)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_scoresgc)\n",
        "                plt.title('Seg-Score-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,8)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_eigensgc)\n",
        "                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n",
        "\n",
        "\n",
        "                plt.subplot(3,3,9)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_ablsgc)\n",
        "                plt.title('Seg-Ablation-CAM', fontsize=fonts)\n",
        "\n",
        "                # Save the full figure...\n",
        "                fig.savefig('./results/SegGradCam_Extensions_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSyIBaWeJTlz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "eo-xai",
      "language": "python",
      "name": "eo-xai"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}